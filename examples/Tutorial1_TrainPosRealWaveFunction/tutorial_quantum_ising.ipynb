{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of a positive-real wavefunction\n",
    "\n",
    "This tutorial shows how to reconstruct a **positive-real** wavefunction via training a *Restricted Boltzmann Machine* (RBM), the neural network behind QuCumber. The data used for training are $\\sigma^{z}$ measurements from a one-dimensional transverse-field Ising model (TFIM) with 10 sites at its critical point.\n",
    "\n",
    "## Transverse-field Ising model\n",
    "The example dataset, located in `tfim1d_data.txt`, comprises 10,000 $\\sigma^{z}$ measurements from a one-dimensional TFIM with 10 sites at its critical point. The Hamiltonian for the TFIM is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathcal{H} = -J\\sum_i \\sigma^z_i \\sigma^z_{i+1} - h \\sum_i\n",
    "\\sigma^x_i\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma^{z}_i$ is the conventional spin-1/2 Pauli operator on site $i$. At the critical point, $J=h=1$. By convention, spins are represented in binary notation with zero and one denoting the states spin-down and spin-up, respectively.\n",
    "\n",
    "## Using QuCumber to reconstruct the wavefunction\n",
    "\n",
    "### Imports\n",
    "To begin the tutorial, first import the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qucumber.nn_states import PositiveWaveFunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python class `PositiveWaveFunction` contains generic properties of a RBM meant to reconstruct a positive-real wavefunction, the most notable one being the gradient function required for stochastic gradient descent.\n",
    "\n",
    "To instantiate a `PositiveWaveFunction` object, one needs to specify the number of visible and hidden units in the RBM. The number of visible units, `num_visible`, is given by the size of the physical system, i.e. the number of spins or qubits (10 in this case), while the number of hidden units, `num_hidden`, can be varied to change the expressiveness of the neural network.\n",
    "\n",
    "**Note:** The optimal `num_hidden` : `num_visible` ratio will depend on the system. For the TFIM, having this ratio be equal to 1 leads to good results with reasonable computational effort.\n",
    "\n",
    "### Training\n",
    "To evaluate the training in real time, we compute the fidelity between the true ground-state wavefunction of the system and the wavefunction that QuCumber reconstructs, $\\vert\\langle\\psi\\vert\\psi_{RBM}\\rangle\\vert^2$, along with the Kullback-Leibler (KL) divergence (the RBM's cost function). As will be shown below, any custom function can be used to evaluate the training.\n",
    "\n",
    "First, the training data and the true wavefunction of this system must be loaded using the *data* utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "psi_path = \"tfim1d_psi.txt\"\n",
    "train_path = \"tfim1d_data.txt\"\n",
    "train_data, true_psi = data.load_data(train_path, psi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, to instantiate a `PositiveWaveFunction` object, one needs to specify the number of visible and hidden units in the RBM; we choose them to be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = train_data.shape[-1]\n",
    "nh = nv\n",
    "\n",
    "nn_state = PositiveWaveFunction(num_visible=nv, num_hidden=nh)\n",
    "# nn_state = PositiveWaveFunction(num_visible=nv, num_hidden=nh, gpu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, QuCumber will attempt to run on a GPU, and default to CPU if GPU is not available. To run QuCumber on a CPU, add the flag `gpu=False` in the `PositiveWaveFunction` object instantiation (i.e. uncomment the line above). \n",
    "\n",
    "Now we specify the hyperparameters of the training process:\n",
    "\n",
    "1. `epochs`: the total number of training cycles that will be performed (default = 100)\n",
    "2. `pbs` (`pos_batch_size`): the number of data points used in the positive phase of the gradient (default = 100)\n",
    "3. `nbs` (`neg_batch_size`): the number of data points used in the negative phase of the gradient (default = 100)\n",
    "4. `k`: the number of contrastive divergence steps (default = 1)\n",
    "5. `lr`: the learning rate (default = 0.001)\n",
    "\n",
    "    **Note:** For more information on the hyperparameters above, it is strongly encouraged that the user to read through the brief, but thorough theory document on RBMs located in the QuCumber documentation. One does not have to specify these hyperparameters, as their default values will be used without the user overwriting them. It is recommended to keep with the default values until the user has a stronger grasp on what these hyperparameters mean. The quality and the computational efficiency of the training will highly depend on the choice of hyperparameters. As such, playing around with the hyperparameters is almost always necessary. \n",
    "    \n",
    "For the TFIM with 10 sites, the following hyperparameters give excellent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "pbs = 100\n",
    "nbs = pbs\n",
    "lr = 0.01\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the training in real time, the `MetricEvaluator` is called every 100 epochs in order to calculate the training evaluators. The `MetricEvaluator` requires the following arguments:\n",
    "\n",
    "1. `period`: the frequency of the training evaluators being calculated (e.g. `period=100` means that the `MetricEvaluator` will do an evaluation every 100 epochs)\n",
    "2. A dictionary of functions you would like to reference to evaluate the training (arguments required for these functions are keyword arguments placed after the dictionary)\n",
    "\n",
    "The following additional arguments are needed to calculate the fidelity and KL divergence in the `training_statistics` utility:\n",
    "\n",
    "- `target_psi`: the true wavefunction of the system\n",
    "- `space`: the Hilbert space of the system\n",
    "\n",
    "The training evaluators can be printed out via the `verbose=True` statement.\n",
    "\n",
    "Although the fidelity and KL divergence are excellent training evaluators, they are not practical to calculate in most cases; the user may not have access to the target wavefunction of the system, nor may generating the Hilbert space of the system be computationally feasible. However, evaluating the training in real time is extremely convenient. \n",
    "\n",
    "Any custom function that the user would like to use to evaluate the training can be given to the `MetricEvaluator`, thus avoiding having to calculate fidelity and/or KL divergence. Any custom function given to `MetricEvaluator` must take the neural-network state (in this case, the `PositiveWaveFunction` object) and keyword arguments. As an example, we define a custom function `psi_coefficient`, which is the fifth coefficient of the reconstructed wavefunction multiplied by a parameter $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_coefficient(nn_state, space, A, **kwargs):\n",
    "    norm = nn_state.compute_normalization(space).sqrt_()\n",
    "    return A * nn_state.psi(space)[0][4] / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Hilbert space of the system can be generated for the fidelity and KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "period = 10\n",
    "space = nn_state.generate_hilbert_space(nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training can begin. The `PositiveWaveFunction` object has a property called `fit` which takes care of this. `MetricEvaluator` must be passed to the `fit` function in a list (`callbacks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    MetricEvaluator(\n",
    "        period,\n",
    "        {\"Fidelity\": ts.fidelity, \"KL\": ts.KL, \"A_Ψrbm_5\": psi_coefficient},\n",
    "        target_psi=true_psi,\n",
    "        verbose=True,\n",
    "        space=space,\n",
    "        A=3.0,\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_state.fit(\n",
    "    train_data,\n",
    "    epochs=epochs,\n",
    "    pos_batch_size=pbs,\n",
    "    neg_batch_size=nbs,\n",
    "    lr=lr,\n",
    "    k=k,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these training evaluators can be accessed after the training has completed. The code below shows this, along with plots of each training evaluator as a function of epoch (training cycle number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the key given to the *MetricEvaluator* must be\n",
    "# what comes after callbacks[0].\n",
    "fidelities = callbacks[0].Fidelity\n",
    "\n",
    "# Alternatively, we can use the usual dictionary/list subsripting\n",
    "# syntax. This is useful in cases where the name of the\n",
    "# metric contains special characters or spaces.\n",
    "KLs = callbacks[0][\"KL\"]\n",
    "coeffs = callbacks[0][\"A_Ψrbm_5\"]\n",
    "\n",
    "epoch = np.arange(period, epochs + 1, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters to make the plots look nice\n",
    "params = {\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"figure.figsize\": (10, 3),\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markeredgewidth\": 0.8,\n",
    "    \"lines.markersize\": 5,\n",
    "    \"lines.marker\": \"o\",\n",
    "    \"patch.edgecolor\": \"black\",\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use(\"seaborn-deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 3))\n",
    "ax = axs[0]\n",
    "ax.plot(epoch, fidelities, \"o\", color=\"C0\", markeredgecolor=\"black\")\n",
    "ax.set_ylabel(r\"Fidelity\")\n",
    "ax.set_xlabel(r\"Epoch\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(epoch, KLs, \"o\", color=\"C1\", markeredgecolor=\"black\")\n",
    "ax.set_ylabel(r\"KL Divergence\")\n",
    "ax.set_xlabel(r\"Epoch\")\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(epoch, coeffs, \"o\", color=\"C2\", markeredgecolor=\"black\")\n",
    "ax.set_ylabel(r\"$A\\psi_{RBM}[5]$\")\n",
    "ax.set_xlabel(r\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fid_KL.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that one could have just ran `nn_state.fit(train_samples)`, which uses the default hyperparameters and no training evaluators.\n",
    "\n",
    "To demonstrate how important it is to find the optimal hyperparameters for a certain system, restart this notebook and comment out the original `fit` statement, then uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_state.fit(train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the non-default hyperparameters yielded a fidelity of approximately $0.989$, while the default hyperparameters yield approximately $0.523$!\n",
    "\n",
    "The trained RBM's parameters are saved to a pickle file with the name `saved_params.pt` for future use in other tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save(\"saved_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the weights, visible biases and hidden biases as torch tensors with the following keys: \"weights\", \"visible_bias\", \"hidden_bias\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
