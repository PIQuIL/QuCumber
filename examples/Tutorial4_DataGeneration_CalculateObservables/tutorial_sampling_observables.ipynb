{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and calculating observables\n",
    "## Generate new samples\n",
    "\n",
    "Firstly, to generate meaningful data, an RBM needs to be trained. Please refer to the tutorials 1 and 2 on training an RBM-based Neural-Network-State if doing so using QuCumber is unclear. A Neural-Network-State (`nn_state`) of a positive-real wavefunction describing a transverse-field Ising model (TFIM) with 10 sites has already been trained in the first tutorial, with the parameters of the machine saved here as `saved_params.pt`. The `autoload` function can be employed here to instantiate the corresponding `PositiveWaveFunction` object from the saved `nn_state` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import qucumber\n",
    "from qucumber.nn_states import PositiveWaveFunction, DensityMatrix\n",
    "from qucumber.observables import ObservableBase\n",
    "from qucumber.observables.pauli import flip_spin\n",
    "from qucumber.utils import cplx\n",
    "\n",
    "from quantum_ising_chain import TFIMChainEnergy, Convergence\n",
    "\n",
    "# set random seed on cpu but not gpu, since we won't use gpu for this tutorial\n",
    "qucumber.set_random_seed(1234, cpu=True, gpu=False)\n",
    "nn_state = PositiveWaveFunction.autoload(\"saved_params.pt\", gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `PositiveWaveFunction` object has a property called `sample` that allows us to sample the learned distribution of TFIM chains. The it takes the following arguments (along with a few others which are not relevant for our purposes):\n",
    "\n",
    "1. `k`: the number of Gibbs steps to perform to generate the new samples. Increasing this number will produce samples closer to the learned distribution, but will require more computation.\n",
    "2. `num_samples`: the number of new data points to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "new_samples = nn_state.sample(k=100, num_samples=10000)\n",
    "print(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnetization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly generated samples, the user can now easily calculate observables that do not require any information associated with the wavefunction and hence the `nn_state`. These are observables which are diagonal in the computational (Pauli Z) basis. A great example of this is the magnetization (in the Z direction). To calculate the magnetization, the newly-generated samples must be converted to $\\pm$ 1 from 1 and 0, respectively. The function below does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pm1(samples):\n",
    "    return samples.mul(2.0).sub(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the (absolute) magnetization in the Z-direction is calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnetization = 0.55752\n"
     ]
    }
   ],
   "source": [
    "def Magnetization(samples):\n",
    "    return to_pm1(samples).mean(1).abs().mean()\n",
    "\n",
    "\n",
    "magnetization = Magnetization(new_samples).item()\n",
    "\n",
    "print(\"Magnetization = %.5f\" % magnetization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact value for the magnetization is 0.5610. \n",
    "\n",
    "The magnetization and the newly-generated samples can also be saved to a pickle file along with the `nn_state` parameters in the *PositiveWaveFunction* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save(\n",
    "    \"saved_params_and_new_data.pt\",\n",
    "    metadata={\"samples\": new_samples, \"magnetization\": magnetization},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` argument in the `save` function takes in a dictionary of data that you would like to save alongside the `nn_state` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate an observable using the *Observable* module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnetization (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuCumber provides the `Observable` module to simplify estimation of expectations and variances of observables in memory efficient ways. To start off, we'll repeat the above example using the `SigmaZ` Observable module provided with QuCumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qucumber.observables import SigmaZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute the absolute magnetization again, for the sake of comparison with the previous example. We want to use the samples drawn earlier to perform this estimate, so we use the `statistics_from_samples` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5575200000000005,\n",
       " 'variance': 0.09791724132414,\n",
       " 'std_error': 0.0031291730748576373,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = SigmaZ(absolute=True)\n",
    "sz.statistics_from_samples(nn_state, new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we get the variance and standard error for free. Now you may be asking: \"That's not too difficult, I could have computed those myself!\". The power of the `Observable` module comes from the fact that it simplifies estimation of these values over a large number of samples. The `statistics` function computes these statistics by generating the samples internally. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 5.83 ms, total: 1.86 s\n",
      "Wall time: 580 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5534800000000003,\n",
       " 'variance': 0.09726161576157935,\n",
       " 'std_error': 0.0031186794603097535,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sz.statistics(nn_state, num_samples=10000, burn_in=100)  \n",
    "# just think of burn_in as being equivalent to k for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider what is taking place under the hood at the moment. The `statistics` function is drawing 10000 samples from the given `nn_state`, and cycling it through the visible and hidden layers for 100 Block Gibbs steps before computing the statistics. This means that, at any given time it has to hold a matrix with 10000 rows and 10 (the number of lattice sites) columns in memory, which becomes infeasible for large lattices or if we want to use more samples to bring our standard error down. To bypass this issue, the `statistics` function allows us to specify the number of Markov Chains to evolve using the `nn_state`, and will sample from these chains multiple times to produce enough samples. It takes the following arguments:\n",
    "\n",
    "- `num_samples`: the number of samples to generate internally\n",
    "- `num_chains`: the number of Markov chains to run in parallel (default = 0, meaning `num_chains` = `num_samples`)\n",
    "- `burn_in`: the number of Gibbs steps to perform before recording any samples (default = 1000)\n",
    "- `steps`: the number of Gibbs steps to perform between each sample; increase this to reduce the autocorrelation between samples (default = 1)\n",
    "- `initial_state`: the initial state of the Markov Chain. If given, `num_chains` will be ignored. (default = `None`)\n",
    "- `overwrite`: Whether to overwrite the `initial_state` tensor, with the updated state of the Markov chain. (default = `False`)\n",
    "\n",
    "The `statistics` function will also return a dictionary containing the mean, standard error (of the mean), the variance, and the total number of samples that were drawn with the keys `mean`, `std_error`, `variance`, and `num_samples` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 419 ms, sys: 6.79 ms, total: 425 ms\n",
      "Wall time: 109 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.55116,\n",
       " 'variance': 0.09716837123712346,\n",
       " 'std_error': 0.003117184165831776,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sz.statistics(nn_state, num_samples=10000, num_chains=1000, burn_in=100, steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, earlier, we had produced a batch of samples `new_samples` which we assumed were already converged to the equilibrium after 100 Gibbs steps. We can use these pre-computed samples to skip the \"burn-in\" phase like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.1 ms, sys: 0 ns, total: 72.1 ms\n",
      "Wall time: 19.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5508999999999998,\n",
       " 'variance': 0.09669885988598853,\n",
       " 'std_error': 0.003109644029241748,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "sz.statistics(\n",
    "    nn_state, num_samples=10000, burn_in=0, steps=2, initial_state=new_samples[:1000, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only took the first 1000 samples from `new_samples` in order to keep the number of Markov Chains the same as the previous cell. Notice how much time was saved by initializing the chains using pre-equilibriated samples. In fact, one could save even more time by skipping the generation of pre-equilibriated samples and using the `nn_state`'s training data instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using less memory (since the matrix held in memory is now of size `num_chains` x `num_sites` = 1000 x 10), using fewer chains also produced a decent speed boost! Next, we'll try increasing the total number of drawn samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.5505633800000023,\n",
       " 'variance': 0.09801013840398955,\n",
       " 'std_error': 9.900006990098014e-05,\n",
       " 'num_samples': 10000000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz.statistics(nn_state, num_samples=int(1e7), num_chains=1000, burn_in=100, steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how much we reduced our standard error just by increasing the number of drawn samples. Finally, we can also draw samples of measurements **of the observable** using the `sample` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 1.0000, 0.0000, 0.4000, 0.2000, 0.4000, 0.4000, 0.4000, 0.6000,\n",
       "        1.0000, 1.0000, 0.6000, 0.4000, 0.0000, 0.4000, 0.4000, 0.8000, 1.0000,\n",
       "        0.0000, 0.4000, 0.0000, 0.0000, 0.8000, 0.8000, 0.8000, 0.8000, 0.6000,\n",
       "        0.8000, 0.6000, 1.0000, 0.4000, 0.4000, 0.4000, 0.4000, 0.8000, 0.2000,\n",
       "        0.2000, 0.0000, 0.8000, 0.4000, 0.6000, 0.0000, 0.2000, 1.0000, 0.4000,\n",
       "        0.8000, 0.2000, 0.4000, 1.0000, 0.6000], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz.sample(nn_state, k=100, num_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this function does not perform any fancy sampling tricks like `statistics` and is therefore susceptible to \"Out of Memory\" errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIM Energy\n",
    "Some observables cannot be computed directly from samples, but instead depend on the `nn_state` as previously mentioned. For example, the magnetization of the TFIM simply depends on the samples the user gives as input. While we did provide the `nn_state` as an argument when calling `statistics_from_samples`, `SigmaZ` ignores it. The TFIM energy, on the other hand, is much more complicated. Consider the TFIM Hamiltonian:\n",
    "\n",
    "$$H = -J\\sum_i \\sigma_i^z \\sigma_{i+1}^z - h\\sum_i \\sigma_i^x$$\n",
    "\n",
    "As our `nn_state` was trained in the Z-basis, the off-diagonal transverse-field term is impossible to compute just from the samples; we need to know the value of the wavefunction for each sample as well. An example for the computation of the energy is provided in the python file `quantum_ising_chain.py`, which takes advantage of QuCumber's `Observable` module.\n",
    "\n",
    "`quantum_ising_chain.py` comprises of a class that computes the energy of a TFIM (`TFIMChainEnergy`) that inherits properties from the `Observable` module. To instantiate a `TFIMChainEnergy` object, the $\\frac{h}{J}$ value must be specified. The trained `nn_state` parameters are from the first tutorial, where the example data was from the TFIM with 10 sites at its critical point ($\\frac{h}{J}=1$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1\n",
    "\n",
    "tfim_energy = TFIMChainEnergy(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go ahead and calculate the mean energy and its standard error from the previously generated samples from this tutorial (`new_samples`), the `statistics_from_samples` function in the `Observable` module is called upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -1.2353 +/- 0.0005\n",
      "Variance: 0.0022\n"
     ]
    }
   ],
   "source": [
    "energy_stats = tfim_energy.statistics_from_samples(nn_state, new_samples)\n",
    "print(\"Mean: %.4f\" % energy_stats[\"mean\"], \"+/- %.4f\" % energy_stats[\"std_error\"])\n",
    "print(\"Variance: %.4f\" % energy_stats[\"variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact value for the energy is -1.2381. \n",
    "\n",
    "To illustrate how quickly the energy converges as a function of the sampling step (i.e. the number of Gibbs steps to perform to generate a new batch of samples), `steps`, the `Convergence` function in `quantum_ising_chain.py` will do the trick. `Convergence` creates a batch of random samples initially, which is then used to generate a new batch of samples from the `nn_state`. The TFIM energy will be calculated at every Gibbs step. Note that this function is not available in the QuCumber API; it is only used here as an illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '% Error in Energy')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = 200\n",
    "num_samples = 10000\n",
    "\n",
    "dict_observables = Convergence(nn_state, tfim_energy, num_samples, steps)\n",
    "\n",
    "energy = dict_observables[\"energies\"]\n",
    "err_energy = dict_observables[\"error\"]\n",
    "\n",
    "step = np.arange(steps + 1)\n",
    "\n",
    "E0 = -1.2381\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(step, abs((E0 - energy) / E0) * 100, color=\"red\")\n",
    "ax.hlines(abs((E0 - energy_stats[\"mean\"]) / E0) * 100, 0, 200, color=\"black\")\n",
    "ax.set_xlim(0, steps)\n",
    "ax.set_ylim(0, 0.6)\n",
    "ax.set_xlabel(\"Gibbs Step\")\n",
    "ax.set_ylabel(\"% Error in Energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see a brief transient period in the magnetization observable, before the state of the machine \"warms up\" to equilibrium (this explains the `burn_in` argument we saw earlier).  After that, the values fluctuate around the estimated mean (the horizontal black line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining observables\n",
    "\n",
    "One may also add / subtract and multiply observables with each other or with real numbers. To illustrate this, we will build an alternative implementation of the TFIM energy observable. First, we will introduce the built-in `NeighbourInteraction` observable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qucumber.observables import NeighbourInteraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFIM chain we trained the `nn_state` on did not have periodic boundary conditions, so `periodic_bcs=False`.\n",
    "Meanwhile, `c` specifies the distance between interacting spins, that is, a given site will only interact with a site `c` places away from itself; we set this to 1 as the TFIM chain has nearest-neighbour interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inter = NeighbourInteraction(periodic_bcs=False, c=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need the `SigmaX` observable, which computes the magnetization in the X-direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qucumber.observables import SigmaX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build the Hamiltonian, setting $h = J = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = J = 1\n",
    "sx = SigmaX()\n",
    "tfim = -J * nn_inter - h * sx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same statistics of this new TFIM observable can also be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -1.2353 +/- 0.0005\n",
      "Variance: 0.0022\n"
     ]
    }
   ],
   "source": [
    "new_tfim_stats = tfim.statistics_from_samples(nn_state, new_samples)\n",
    "print(\"Mean: %.4f\" % new_tfim_stats[\"mean\"], \"+/- %.4f\" % new_tfim_stats[\"std_error\"])\n",
    "print(\"Variance: %.4f\" % new_tfim_stats[\"variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics above match with those computed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rényi Entropy and the Swap operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the second Rényi Entropy using the Swap operator as shown by [Hastings et al. (2010)](https://link.aps.org/doi/10.1103/PhysRevLett.104.157201).\n",
    "The second Rényi Entropy, in terms of the expectation of the Swap operator is given by:\n",
    "\n",
    "$$S_2(A) = -\\ln\\langle \\text{Swap}_A \\rangle$$\n",
    "\n",
    "where $A$ is the subset of the lattice for which we wish to compute the Rényi entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qucumber.observables import SWAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will take the region $A$ consist of sites $0$ through $4$ (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7838 +/- 0.0061\n",
      "Variance: 0.3663\n"
     ]
    }
   ],
   "source": [
    "A = [0, 1, 2, 3, 4]\n",
    "swap = SWAP(A)\n",
    "\n",
    "swap_stats = swap.statistics_from_samples(nn_state, new_samples)\n",
    "print(\"Mean: %.4f\" % swap_stats[\"mean\"], \"+/- %.4f\" % swap_stats[\"std_error\"])\n",
    "print(\"Variance: %.4f\" % swap_stats[\"variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second Rényi Entropy can be computed directly from the sample mean. The standard error of the entropy, from first-order error analysis, is given by the standard error of the Swap operator divided by the mean of the Swap operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_2: 0.2437 +/- 0.0077\n"
     ]
    }
   ],
   "source": [
    "S_2 = -np.log(swap_stats[\"mean\"])\n",
    "S_2_error = abs(swap_stats[\"std_error\"] / swap_stats[\"mean\"])\n",
    "\n",
    "\n",
    "print(\"S_2: %.4f\" % S_2, \"+/- %.4f\" % S_2_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing custom diagonal observables\n",
    "QuCumber has a built-in module called `Observable` which makes it easy for the user to compute any arbitrary observable from the `nn_state`. To see the the `Observable` module in action, an example (diagonal) observable called `PIQuIL`, which inherits properties from the `Observable` module, is shown below. \n",
    "\n",
    "The `PIQuIL` observable takes a $\\sigma^z$ measurement at a site and multiplies it by the measurement two sites away from it. There is also a parameter, $P$ , that determines the strength of each of these interactions. For example, for the dataset $(-1,1,1,-1), (1,1,1,1)$ and $(1,1,-1,1)$ with $P = 2$, the `PIQuIL` for each data point would be $\\left( 2(-1\\times1) + 2(1\\times-1) = -4 \\right), \\left( 2(1\\times1) + 2(1\\times1) = 4 \\right)$ and $\\left( 2(1\\times-1) + 2(1\\times1) = 0 \\right)$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIQuIL(ObservableBase):\n",
    "    def __init__(self, P):\n",
    "        self.name = \"PIQuIL\"\n",
    "        self.symbol = \"Q\"\n",
    "        self.P = P\n",
    "\n",
    "    # Required : function that calculates the PIQuIL. Must be named \"apply\"\n",
    "    def apply(self, nn_state, samples):\n",
    "        samples = to_pm1(samples)\n",
    "        interaction_ = 0.0\n",
    "        for i in range(samples.shape[-1] - 2):\n",
    "            interaction_ += self.P * samples[:, i] * samples[:, i + 2]\n",
    "\n",
    "        return interaction_\n",
    "\n",
    "\n",
    "P = 0.05\n",
    "piquil = PIQuIL(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` function is contained in the `Observable` module, but is overwritten here. The `apply` function in `Observable` will compute the observable itself and must take in the `nn_state` and a batch of samples as arguments. Thus, any new class inheriting from `Observable` that the user would like to define must contain a function called `apply` that calculates this new observable. For more details on `apply`, we refer to the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the `PIQuIL` observable could technically be computed without the first argument of `apply` since it does not ever use the `nn_state`, we still include it in the list of arguments in order to conform to the interface provided in the `ObservableBase` class.\n",
    "\n",
    "Since we have already generated new samples of data, the `PIQuIL` observable's mean, standard error and variance on the new data can be calculated with the `statistics_from_samples` function in the `Observable` module. The user must simply provide the `nn_state` and the samples as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "piquil_stats1 = piquil.statistics_from_samples(nn_state, new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statistics_from_samples` function returns a dictionary containing the mean, standard error and the variance with the keys \"mean\", \"std_error\" and \"variance\", respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean PIQuIL: 0.1762 +/- 0.0016\n",
      "Variance: 0.0244\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Mean PIQuIL: %.4f\" % piquil_stats1[\"mean\"], \"+/- %.4f\" % piquil_stats1[\"std_error\"]\n",
    ")\n",
    "print(\"Variance: %.4f\" % piquil_stats1[\"variance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: We notice that the `PIQuIL` observable is essentially a scaled next-nearest-neighbours interaction. \n",
    "(a) Construct an equivalent `Observable` object algebraically in a similar manner to the TFIM observable constructed above.\n",
    "(b) Compute the statistics of this observable on `new_samples`, and compare to those computed using the `PIQuIL` observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the above exercise here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing off-diagonal observables\n",
    "Now, as the `PIQuIL` observable was diagonal, it was fairly easy to write. Things get a bit more complicated once we consider off-diagonal observables, as we'd need to make use of information about the quantum state itself.\n",
    "In general, computing an observable exactly with respect to the state $\\rho$ requires performing a trace:\n",
    "\n",
    "$$\n",
    "\\langle O \\rangle = \\text{Tr}\\,\\lbrack O \\rho \\rbrack\n",
    "= \\sum_i \\langle i \\vert O \\rho \\vert i \\rangle\n",
    "= \\sum_{ij} \\langle i \\vert O \\vert j \\rangle \\langle j \\vert \\rho \\vert i \\rangle\n",
    "$$\n",
    "\n",
    "where $\\lbrace\\vert i \\rangle\\rbrace_i, \\lbrace\\vert j \\rangle\\rbrace_j$ are two orthonormal bases spanning the Hilbert space. Multiplying the numerator and denominator by $\\langle i \\vert \\rho \\vert i \\rangle$ gives:\n",
    "\n",
    "$$\n",
    "\\langle O \\rangle = \\sum_{i} \\langle i \\vert \\rho \\vert i \\rangle \\sum_j \\frac{\\langle j \\vert \\rho \\vert i \\rangle}{\\langle i \\vert \\rho \\vert i \\rangle}\\langle i \\vert O \\vert j \\rangle = \n",
    "\\sum_{i} \\rho_{ii} \\sum_j \\frac{\\rho_{ji}}{\\rho_{ii}} O_{ij}\n",
    "= \\sum_i \\rho_{ii} \\mathcal{O}_{i}\n",
    "$$\n",
    "\n",
    "Hence, computing the expectation of the observable $O$ with respect to $\\rho$, amounts to estimating the so-called \"local-estimator\" $\\mathcal{O}$ with respect to the probability distribution $\\lbrace \\rho_{ii} \\rbrace_i$. Setting $\\lbrace\\vert i \\rangle\\rbrace_i$ to our computational basis states $\\lbrace \\vert \\sigma \\rangle \\rbrace$, we note that, as we are able to draw samples from $\\rho$ in the computational basis using our `nn_state`, we can easily estimate the expectation of $O$:\n",
    "\n",
    "$$\n",
    "\\langle O \\rangle = \\sum_{\\sigma} \\rho_{\\sigma\\sigma} \\mathcal{O}(\\sigma)\n",
    "\\approx \\frac{1}{\\vert\\mathcal{D}\\vert} \\sum_{\\sigma \\in \\mathcal{D}} \\mathcal{O}(\\sigma)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}$ denotes the set of drawn samples. Recall that the local-estimator is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(\\sigma) = \\sum_{\\sigma'} \\frac{\\rho(\\sigma', \\sigma)}{\\rho(\\sigma,\\sigma)} O(\\sigma, \\sigma')\n",
    "$$\n",
    "\n",
    "which, in the case of a pure state $\\rho = \\vert \\psi \\rangle\\langle \\psi \\vert$, reduces to:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(\\sigma) = \\sum_{\\sigma'} \\frac{\\psi(\\sigma')}{\\psi(\\sigma)} O(\\sigma, \\sigma')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of the `apply` function, is actually to compute the local-estimator, given a sample $\\sigma$. Ideally, this function would take into account the structure of $O$ in order to perform this computation efficiently, and avoid iterating through every entry of the wavefunction of density matrix unnecessarily.\n",
    "\n",
    "It should be noted that, though the Neural-Network-States provided by QuCumber do not give normalized probability estimates, this is not an issue for computing the local-estimator, as the normalization constant cancels out.\n",
    "\n",
    "As an example, we will write a simplified version of the `SigmaX` observable. But first, let's see what the statistics of the official version of `SigmaX` are, for the sake of later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.7293210861294865,\n",
       " 'variance': 0.07933831206407158,\n",
       " 'std_error': 0.002816705736566594,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.statistics_from_samples(nn_state, new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmaX(ObservableBase):\n",
    "    def __init__(self):\n",
    "        self.name = \"SigmaX\"\n",
    "        self.symbol = \"X\"\n",
    "\n",
    "    def apply(self, nn_state, samples):\n",
    "        samples = samples.to(device=nn_state.device)\n",
    "\n",
    "        # vectors of shape: (2, num_samples,)\n",
    "        denom = cplx.conjugate(nn_state.psi(samples))\n",
    "        numer_sum = torch.zeros_like(denom)\n",
    "\n",
    "        for i in range(samples.shape[-1]):  # sum over spin sites\n",
    "            samples_ = flip_spin(i, samples.clone())  # flip the spin at site i\n",
    "\n",
    "            # compute the numerator of the importance and add it to the running sum\n",
    "            numer = cplx.conjugate(nn_state.psi(samples_))\n",
    "            numer_sum.add_(numer)\n",
    "\n",
    "        mag = cplx.elementwise_division(numer_sum, denom)\n",
    "\n",
    "        # take real part (imaginary part should be approximately zero)\n",
    "        # and divide by number of spins\n",
    "        return cplx.real(mag).div_(samples.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.7293210861294865,\n",
       " 'variance': 0.07933831206407158,\n",
       " 'std_error': 0.002816705736566594,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MySigmaX().statistics_from_samples(nn_state, new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're on the right track! The only remaining problem is generalizing this to work with mixed states. Note that in both expressions of the local-estimator, we need to compute a ratio dependent on $\\sigma$ and $\\sigma'$. The Neural-Network-States provided by QuCumber implement the functions `importance_sampling_weight`, `importance_sampling_numerator`, and `importance_sampling_denominator` in order to simplify writing observables for both pure and mixed states. \n",
    "\n",
    "In simple cases, we'd only need to make use of `importance_sampling_weight`, however, note that, since the denominator can be factored out of the summation, it is more efficient to compute the numerator and denominator separately in order to avoid duplicating work. Let's update our version of the X-magnetization observable to support mixed states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmaX(ObservableBase):\n",
    "    def __init__(self):\n",
    "        self.name = \"SigmaX\"\n",
    "        self.symbol = \"X\"\n",
    "\n",
    "    def apply(self, nn_state, samples):\n",
    "        samples = samples.to(device=nn_state.device)\n",
    "\n",
    "        # vectors of shape: (2, num_samples,)\n",
    "        denom = nn_state.importance_sampling_denominator(samples)\n",
    "        numer_sum = torch.zeros_like(denom)\n",
    "\n",
    "        for i in range(samples.shape[-1]):  # sum over spin sites\n",
    "            samples_ = flip_spin(i, samples.clone())  # flip the spin at site i\n",
    "\n",
    "            # compute the numerator of the importance and add it to the running sum\n",
    "            numer = nn_state.importance_sampling_numerator(samples_, samples)\n",
    "            numer_sum.add_(numer)\n",
    "\n",
    "        mag = cplx.elementwise_division(numer_sum, denom)\n",
    "\n",
    "        # take real part (imaginary part should be approximately zero)\n",
    "        # and divide by number of spins\n",
    "        return cplx.real(mag).div_(samples.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.7293210861294865,\n",
       " 'variance': 0.07933831206407158,\n",
       " 'std_error': 0.002816705736566594,\n",
       " 'num_samples': 10000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MySigmaX().statistics_from_samples(nn_state, new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not much has actually changed in our code. In fact, one can often write local-estimators for observables assuming a pure-state, and then later easily generalize their code to support mixed states using the abstract functions discussed earlier. As a final sanity check, let's try estimating the statistics of `MySigmaX` for a randomly initialized `DensityMatrix`, and compare the output to that of the official implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean': 0.9930701314464155,\n",
       "  'variance': 0.010927188103705533,\n",
       "  'std_error': 0.0010453319139730468,\n",
       "  'num_samples': 10000},\n",
       " {'mean': 0.9930701314464155,\n",
       "  'variance': 0.010927188103705533,\n",
       "  'std_error': 0.0010453319139730468,\n",
       "  'num_samples': 10000})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_nn_state = DensityMatrix(nn_state.num_visible, gpu=False)\n",
    "\n",
    "(\n",
    "    sx.statistics_from_samples(mixed_nn_state, new_samples),\n",
    "    MySigmaX().statistics_from_samples(mixed_nn_state, new_samples),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Statistics of Many Observables Simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may often be concerned with estimating the statistics of many observables simultaneously. In order to avoid excess memory usage, it makes sense to reuse the same set of samples to estimate each observable. When we need a large number of samples however, we run into the same issue mentioned earlier: we may run out of memory storing the samples. QuCumber provides a `System` object to keep track of multiple observables and estimate their statistics efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qucumber.observables import System\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we must make a quick aside: internally, `System` keeps track of multiple observables through their `name` field (which we saw in the definition of the `PIQuIL` observable). This name is returned by Python's built-in `repr` function, which is automatically called when we try to display an `Observable` object in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIQuIL"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piquil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-1 * NeighbourInteraction(periodic_bcs=False, c=1)) + -(1 * SigmaX))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the TFIM energy observable's name is quite complicated, due to the fact that we constructed it algebraically as opposed to the `PIQuIL` observable which was built from scratch and manually assigned a name.\n",
    "In order to assign a name to `tfim`, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFIM"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfim.name = \"TFIM\"\n",
    "tfim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to `System`. We'd like to create a `System` object which keeps track of the absolute magnetization, the energy of the chain, the Swap observable (of region $A$, as defined earlier), and finally, the `PIQuIL` observable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfim_system = System(sz, tfim, swap, piquil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIQuIL': {'mean': 0.1762100000000003,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.001561328717706924,\n",
      "            'variance': 0.024377473647363472},\n",
      " 'SWAP': {'mean': 0.7837510693478925,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.006052467264446535,\n",
      "          'variance': 0.3663235998719692},\n",
      " 'SigmaZ': {'mean': 0.5575200000000005,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.0031291730748576373,\n",
      "            'variance': 0.09791724132414},\n",
      " 'TFIM': {'mean': -1.2352610861294844,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.0004669027817740233,\n",
      "          'variance': 0.002179982076283212}}\n"
     ]
    }
   ],
   "source": [
    "pprint(tfim_system.statistics_from_samples(nn_state, new_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all match with the values computed earlier. Next, we will compute these statistics from fresh samples drawn from the `nn_state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIQuIL': {'mean': 0.17354,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.001551072990784856,\n",
      "            'variance': 0.02405827422742278},\n",
      " 'SWAP': {'mean': 0.7824233758221596,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.006105647043859781,\n",
      "          'variance': 0.3727892582419368},\n",
      " 'SigmaZ': {'mean': 0.5508199999999999,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.0031138886284257233,\n",
      "            'variance': 0.09696302390239034},\n",
      " 'TFIM': {'mean': -1.2351832610706792,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.00046588592121667226,\n",
      "          'variance': 0.002170496915879073}}\n",
      "CPU times: user 774 ms, sys: 7.94 ms, total: 782 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pprint(\n",
    "    tfim_system.statistics(\n",
    "        nn_state, num_samples=10000, num_chains=1000, burn_in=100, steps=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to computing these statistics on each observable individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIQuIL': {'mean': 0.17636999999999997,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.0015554112769374036,\n",
      "            'variance': 0.024193042404240445},\n",
      " 'SWAP': {'mean': 0.7788363185216998,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.005804524946475529,\n",
      "          'variance': 0.3369250985425674},\n",
      " 'SigmaZ': {'mean': 0.55804,\n",
      "            'num_samples': 10000,\n",
      "            'std_error': 0.0031169387820002294,\n",
      "            'variance': 0.09715307370737074},\n",
      " 'TFIM': {'mean': -1.2345632953955774,\n",
      "          'num_samples': 10000,\n",
      "          'std_error': 0.0004844496452134716,\n",
      "          'variance': 0.0023469145874745853}}\n",
      "CPU times: user 1.67 s, sys: 7.82 ms, total: 1.68 s\n",
      "Wall time: 424 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pprint(\n",
    "    {\n",
    "        obs.name: obs.statistics(\n",
    "            nn_state, num_samples=10000, num_chains=1000, burn_in=100, steps=2\n",
    "        )\n",
    "        for obs in [piquil, swap, sz, tfim]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the slowdown. This is, as mentioned before, due to the fact that the `System` object uses *the same samples* to estimate statistics for *all* of the observables it is keeping track of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Template for your custom observable\n",
    "Here is a generic template for you to try using the `Observable` module yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourObservable(ObservableBase):\n",
    "    def __init__(self, your_constants):\n",
    "        self.your_constants = your_constants\n",
    "        self.name = \"Observable_Name\"\n",
    "\n",
    "        # The algebraic symbol representing this Observable.\n",
    "        # Returned by Python's built-in str() function\n",
    "        self.symbol = \"O\"\n",
    "\n",
    "    def apply(self, nn_state, samples):\n",
    "        # arguments of \"apply\" must be in this order\n",
    "\n",
    "        # calculate your observable for each data point\n",
    "        obs = torch.tensor([42] * len(samples))\n",
    "\n",
    "        # make sure the observables are on the same device and have the\n",
    "        # same dtype as the samples\n",
    "        obs = obs.to(samples)\n",
    "\n",
    "        # return a torch tensor containing the observable values\n",
    "        return obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
