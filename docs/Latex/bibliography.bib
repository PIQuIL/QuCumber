%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Alexandre Dauphin at 2017-10-10 15:11:17 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{Gao,
	Abstract = {Part of the challenge for quantum many-body problems comes from the difficulty of representing large-scale quantum states, which in general requires an exponentially large number of parameters. Neural networks provide a powerful tool to represent quantum many-body states. An important open question is what characterizes the representational power of deep and shallow neural networks, which is of fundamental interest due to the popularity of deep learning methods. Here, we give a proof that, assuming a widely believed computational complexity conjecture, a deep neural network can efficiently represent most physical states, including the ground states of many-body Hamiltonians and states generated by quantum dynamics, while a shallow network representation with a restricted Boltzmann machine cannot efficiently represent some of those states.},
	Author = {Gao, Xun and Duan, Lu-Ming},
	Da = {2017/09/22},
	Date-Added = {2018-10-03 14:28:31 +0000},
	Date-Modified = {2018-10-03 14:28:31 +0000},
	Doi = {10.1038/s41467-017-00705-2},
	Id = {Gao2017},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {662},
	Title = {Efficient representation of quantum many-body states with deep neural networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-017-00705-2},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-017-00705-2}}




@comment{jabref-meta: databaseType:bibtex;}

}
@article{TorlaiMixed,
  title = {Latent Space Purification via Neural Density Operators},
  author = {Torlai, Giacomo and Melko, Roger G.},
  journal = {Phys. Rev. Lett.},
  volume = {120},
  issue = {24},
  pages = {240503},
  numpages = {5},
  year = {2018},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.120.240503},
  url = {https://link.aps.org/doi/10.1103/Phys Revet.120.240503}
}


@article{Maciej,
	Abstract = {Physical systems differing in their microscopic details often display strikingly similar behaviour when probed at macroscopic scales. Those universal properties, largely determining their physical characteristics, are revealed by the powerful renormalization group (RG) procedure, which systematically retains `slow'degrees of freedom and integrates out the rest. However, the important degrees of freedom may be difficult to identify. Here we demonstrate a machine-learning algorithm capable of identifying the relevant degrees of freedom and executing RG steps iteratively without any prior knowledge about the system. We introduce an artificial neural network based on a model-independent, information-theoretic characterization of a real-space RG procedure, which performs this task. We apply the algorithm to classical statistical physics problems in one and two dimensions. We demonstrate RG flow and extract the Ising critical exponent. Our results demonstrate that machine-learning techniques can extract abstract physical concepts and consequently become an integral part of theory- and model-building.},
	Author = {Koch-Janusz, Maciej and Ringel, Zohar},
	Da = {2018/06/01},
	Date-Added = {2018-09-24 19:58:38 +0000},
	Date-Modified = {2018-09-24 19:58:38 +0000},
	Doi = {10.1038/s41567-018-0081-4},
	Id = {Koch-Janusz2018},
	Isbn = {1745-2481},
	Journal = {Nature Physics},
	Number = {6},
	Pages = {578--582},
	Title = {Mutual information, neural networks and the renormalization group},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41567-018-0081-4},
	Volume = {14},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41567-018-0081-4},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/s41567-018-0081-4}}


@article{itensor,
    note={Calculations were performed using the ITensor Library (version 0.2.4)},
    title={http://itensor.org/},
    url={http://itensor.org/}
    }

@inproceedings{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@article{zwerger2003mott,
	Abstract = {We discuss the superfluid (SF) to Mott-insulator transition of cold atoms in optical lattices recently observed by Greiner et al (2002 Nature 415 39). The fundamental properties of both phases and their experimental signatures are discussed carefully, including the limitations of the standard Gutzwiller approximation. It is shown that in a one-dimensional dilute Bose-gas with a strong transverse confinement (Tonks-gas), even an arbitrary weak optical lattice is able to induce a Mott-like state with crystalline order, provided the dimensionless interaction parameter is larger than a critical value of order one. The SF--insulator transition of the Bose--Hubbard model in this case continuously evolves into a transition of the commensurate--incommensurate type with decreasing strength of the external optical lattice.},
	Author = {Wilhelm Zwerger},
	Date-Added = {2017-10-10 13:10:51 +0000},
	Date-Modified = {2017-10-10 13:11:17 +0000},
	Journal = {Journal of Optics B: Quantum and Semiclassical Optics},
	Number = {2},
	Pages = {S9},
	Title = {{Mott--Hubbard} transition of cold atoms in optical lattices},
	Volume = {5},
	Year = {2003},
	doi = {10.1088/1464-4266/5/2/352},
	Bdsk-Url-1 = {http://stacks.iop.org/1464-4266/5/i=2/a=352}}

@article{Torlai2016thermo,
  title = {Learning thermodynamics with Boltzmann machines},
  author = {Torlai, Giacomo and Melko, Roger G.},
  journal = {Phys. Rev. B},
  volume = {94},
  issue = {16},
  pages = {165134},
  numpages = {7},
  year = {2016},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.94.165134},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.94.165134}
}

@article{torlai2018tomography,
  title={Neural-network quantum state tomography},
  author={Torlai, Giacomo and Mazzola, Guglielmo and Carrasquilla, Juan and Troyer, Matthias and Melko, Roger and Carleo, Giuseppe},
  journal={Nature Physics},
  volume={14},
  doi= {10.1038/s41567-018-0048-5},
  number={5},
  pages={447},
  year={2018},
  publisher={Nature Publishing Group}
}

@article {CarleoTroyer2017Science,
	author = {Carleo, Giuseppe and Troyer, Matthias},
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	number = {6325},
	pages = {602--606},
	year = {2017},
	doi = {10.1126/science.aag2302},
	publisher = {American Association for the Advancement of Science},
	abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.Science, this issue p. 602; see also p. 580The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/355/6325/602},
	eprint = {http://science.sciencemag.org/content/355/6325/602.full.pdf},
	journal = {Science}
}

@book{Smolensky,
author={Paul Smolensky},
year={1986},
chapter={6},
title={Information Processing in Dynamical Systems: Foundations of Harmony Theory},
pages={194–281},
editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
isbn = {0-262-68053-X},
publisher = {MIT Press},
address = {Cambridge, MA, USA} 
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  doi = {10.1162/089976602760128018},
  year={2002},
  publisher={MIT Press}
}

@incollection{hinton2012practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey E},
  booktitle={Neural networks: Tricks of the trade},
  pages={599--619},
  doi={10.1007/978-3-642-35289-8_32 },
  year={2012},
  publisher={Springer}
}

@article{GlasserCirac2018,
  title = {Neural-Network Quantum States, String-Bond States, and Chiral Topological States},
  author = {Glasser, Ivan and Pancotti, Nicola and August, Moritz and Rodriguez, Ivan D. and Cirac, J. Ignacio},
  journal = {Phys. Rev. X},
  volume = {8},
  issue = {1},
  pages = {011006},
  numpages = {16},
  year = {2018},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.8.011006},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.8.011006}
}

@article{ChenWang2018,
  title = {Equivalence of restricted Boltzmann machines and tensor network states},
  author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
  journal = {Phys. Rev. B},
  volume = {97},
  issue = {8},
  pages = {085104},
  numpages = {16},
  year = {2018},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.97.085104},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104}
}

@article{Hinton02,
author = {Hinton, Geoffrey E.},
title = {Training Products of Experts by Minimizing Contrastive Divergence},
journal = {Neural Computation},
volume = {14},
number = {8},
pages = {1771-1800},
year = {2002},
doi = {10.1162/089976602760128018},

URL = { 
        https://doi.org/10.1162/089976602760128018
    
},
eprint = { 
        https://doi.org/10.1162/089976602760128018}}
        
        
        @article{gambetta18,
  author={Nikolaj Moll and Panagiotis Barkoutsos and Lev S Bishop and Jerry M Chow and Andrew Cross and Daniel J Egger and Stefan
Filipp and Andreas Fuhrer and Jay M Gambetta and Marc Ganzhorn and Abhinav Kandala and Antonio Mezzacapo and Peter
Müller and Walter Riess and Gian Salis and John Smolin and Ivano Tavernelli and Kristan Temme},
  title={Quantum optimization using variational algorithms on near-term quantum devices},
  journal={Quantum Science and Technology},
  volume={3},
  number={3},
  pages={030503},
  url={http://stacks.iop.org/2058-9565/3/i=3/a=030503},
  year={2018},
  abstract={Universal fault-tolerant quantum computers will require error-free execution of long sequences of quantum gate operations, which is expected to involve millions of physical qubits. Before the full power of such machines will be available, near-term quantum devices will provide several hundred qubits and limited error correction. Still, there is a realistic prospect to run useful algorithms within the limited circuit depth of such devices. Particularly promising are optimization algorithms that follow a hybrid approach: the aim is to steer a highly entangled state on a quantum system to a target state that minimizes a cost function via variation of some gate parameters. This variational approach can be used both for classical optimization problems as well as for problems in quantum chemistry. The challenge is to converge to the target state given the limited coherence time and connectivity of the qubits. In this context, the quantum volume as a metric to compare the power of near-term quantum devices is discussed. With focus on chemistry applications, a general description of variational algorithms is provided and the mapping from fermions to qubits is explained. Coupled-cluster and heuristic trial wave-functions are considered for efficiently finding molecular ground states. Furthermore, simple error-mitigation schemes are introduced that could improve the accuracy of determining ground-state energies. Advancing these techniques may lead to near-term demonstrations of useful quantum computation with systems containing several hundred qubits.}
}

@article{Hinton06,
  title={A Fast Learning Algorithm for Deep Belief Nets},
  author={G. Hinton and S. Osindero and Y. Teh},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press},
  url = "http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527#.VzSfdWamvEY"
}
@article {Hinton504,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	number = {5786},
	pages = {504--507},
	year = {2006},
	doi = {10.1126/science.1127647},
	publisher = {American Association for the Advancement of Science},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/313/5786/504},
	eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
	journal = {Science}
}

	
@article{gambetta17,
	Author = {Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
	Date = {2017/09/13/online},
	Date-Added = {2018-09-11 19:43:16 +0000},
	Date-Modified = {2018-09-11 19:43:16 +0000},
	Day = {13},
	Journal = {Nature},
	L3 = {10.1038/nature23879; https://www.nature.com/articles/nature23879#supplementary-information},
	Month = {09},
	Pages = {242 EP  -},
	Publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
	Title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nature23879},
	Volume = {549},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nature23879}}

@article{PhysRevLett.119.030501,
  title = {Neural Decoder for Topological Codes},
  author = {Torlai, Giacomo and Melko, Roger G.},
  journal = {Phys. Rev. Lett.},
  volume = {119},
  issue = {3},
  pages = {030501},
  numpages = {5},
  year = {2017},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.119.030501},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.030501}
}



@article{Bernien17,
	Author = {Bernien, Hannes and Schwartz, Sylvain and Keesling, Alexander and Levine, Harry and Omran, Ahmed and Pichler, Hannes and Choi, Soonwon and Zibrov, Alexander S. and Endres, Manuel and Greiner, Markus and Vuleti{\'c}, Vladan and Lukin, Mikhail D.},
	Date = {2017/11/29/online},
	Date-Added = {2018-09-06 15:04:24 +0000},
	Date-Modified = {2018-09-06 15:04:24 +0000},
	Day = {29},
	Journal = {Nature},
	L3 = {10.1038/nature24622; },
	M3 = {Article},
	Month = {11},
	Pages = {579 EP  -},
	Publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
	Title = {Probing many-body dynamics on a 51-atom quantum simulator},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nature24622},
	Volume = {551},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nature24622}}


@article{PhysRevB.95.035105,
  title = {Accelerated Monte Carlo simulations with restricted Boltzmann machines},
  author = {Huang, Li and Wang, Lei},
  journal = {Phys. Rev. B},
  volume = {95},
  issue = {3},
  pages = {035105},
  numpages = {6},
  year = {2017},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.95.035105},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.95.035105}
}
@article{PhysRevB.95.041101,
  title = {Self-learning Monte Carlo method},
  author = {Liu, Junwei and Qi, Yang and Meng, Zi Yang and Fu, Liang},
  journal = {Phys. Rev. B},
  volume = {95},
  issue = {4},
  pages = {041101},
  numpages = {5},
  year = {2017},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.95.041101},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.95.041101}
}

