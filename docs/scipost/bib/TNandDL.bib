
@article{stoudenmire_learning_2018,
	title = {Learning relevant features of data with multi-scale tensor networks},
	volume = {3},
	issn = {2058-9565},
	url = {http://stacks.iop.org/2058-9565/3/i=3/a=034003},
	abstract = {Inspired by coarse-graining approaches used in physics, we show how similar algorithms can be adapted for data. The resulting algorithms are based on layered tree tensor networks and scale linearly with both the dimension of the input and the training set size. Computing most of the layers with an unsupervised algorithm, then optimizing just the top layer for supervised classification of the MNIST and fashion MNIST data sets gives very good results. We also discuss mixing a prior guess for supervised weights together with an unsupervised representation of the data, yielding a smaller number of features nevertheless able to give good performance.},
	language = {en},
	urldate = {2018-12-05},
	journal = {Quantum Sci. Technol.},
	author = {Stoudenmire, E. Miles},
	year = {2018},
	pages = {034003},
	file = {Submitted Version:/home/meach/Zotero/storage/UI5ZKRXZ/Stoudenmire - 2018 - Learning relevant features of data with multi-scal.pdf:application/pdf}
}

@article{huggins_towards_2018,
	title = {Towards {Quantum} {Machine} {Learning} with {Tensor} {Networks}},
	url = {https://arxiv.org/abs/1803.11537},
	language = {en},
	urldate = {2018-12-05},
	author = {Huggins, William and Patel, Piyush and Whaley, K. Birgitta and Stoudenmire, E. Miles},
	month = mar,
	year = {2018},
	file = {Full Text PDF:/home/meach/Zotero/storage/GA5H7EYE/Huggins et al. - 2018 - Towards Quantum Machine Learning with Tensor Netwo.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/YWHZC66L/1803.html:text/html}
}

@inproceedings{stoudenmire_supervised_2016,
	title = {Supervised {Learning} with {Tensor} {Networks}},
	url = {http://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf},
	urldate = {2018-12-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Stoudenmire, Edwin and Schwab, David J},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4799--4807},
	file = {NIPS Full Text PDF:/home/meach/Zotero/storage/BAUL9W3G/Stoudenmire and Schwab - 2016 - Supervised Learning with Tensor Networks.pdf:application/pdf;NIPS Snapshot:/home/meach/Zotero/storage/4H7S2STX/6211-supervised-learning-with-tensor-networks.html:text/html}
}

@article{xu_neural_2018,
	title = {Neural network state estimation for full quantum state tomography},
	url = {https://arxiv.org/abs/1811.06654},
	language = {en},
	urldate = {2018-12-05},
	author = {Xu, Qian and Xu, Shuqi},
	month = nov,
	year = {2018},
	file = {Full Text PDF:/home/meach/Zotero/storage/W7A4JCR4/Xu and Xu - 2018 - Neural network state estimation for full quantum s.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/UD6ECVAS/1811.html:text/html}
}

@article{newman_stable_2018,
	title = {Stable {Tensor} {Neural} {Networks} for {Rapid} {Deep} {Learning}},
	url = {https://arxiv.org/abs/1811.06569v1},
	language = {en},
	urldate = {2018-12-05},
	author = {Newman, Elizabeth and Horesh, Lior and Avron, Haim and Kilmer, Misha},
	month = nov,
	year = {2018},
	file = {Full Text PDF:/home/meach/Zotero/storage/S8HPCNV2/Newman et al. - 2018 - Stable Tensor Neural Networks for Rapid Deep Learn.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/N943XLM9/1811.html:text/html}
}

@article{chen_equivalence_2018,
	title = {Equivalence of restricted {Boltzmann} machines and tensor network states},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104},
	abstract = {The restricted Boltzmann machine (RBM) is one of the fundamental building blocks of deep learning. RBM finds wide applications in dimensional reduction, feature extraction, and recommender systems via modeling the probability distributions of a variety of input data including natural images, speech signals, and customer ratings, etc. We build a bridge between RBM and tensor network states (TNS) widely used in quantum many-body physics research. We devise efficient algorithms to translate an RBM into the commonly used TNS. Conversely, we give sufficient and necessary conditions to determine whether a TNS can be transformed into an RBM of given architectures. Revealing these general and constructive connections can cross fertilize both deep learning and quantum many-body physics. Notably, by exploiting the entanglement entropy bound of TNS, we can rigorously quantify the expressive power of RBM on complex data sets. Insights into TNS and its entanglement capacity can guide the design of more powerful deep learning architectures. On the other hand, RBM can represent quantum many-body states with fewer parameters compared to TNS, which may allow more efficient classical simulations.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. B},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	month = feb,
	year = {2018},
	pages = {085104},
	file = {APS Snapshot:/home/meach/Zotero/storage/SJRMN3DY/PhysRevB.97.html:text/html;Submitted Version:/home/meach/Zotero/storage/KSKH9TS6/Chen et al. - 2018 - Equivalence of restricted Boltzmann machines and t.pdf:application/pdf}
}
